{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomik062/AI_Project/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "extract data from github and merge together"
      ],
      "metadata": {
        "id": "QoOciPbUXoL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import importlib.util\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV, LeaveOneOut,KFold\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Lasso,ElasticNet\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "\n",
        "# Define the GitHub details\n",
        "def init_process():\n",
        "  repo_owner = 'tomik062'\n",
        "  repo_name = 'AI_Project'\n",
        "  file_path = 'extract_data.py'\n",
        "  url = f'https://raw.githubusercontent.com/{repo_owner}/{repo_name}/main/{file_path}'\n",
        "\n",
        "  # Directory to save the downloaded file\n",
        "  download_dir = 'data_extraction_code'\n",
        "  if not os.path.exists(download_dir):\n",
        "      os.makedirs(download_dir)\n",
        "\n",
        "  local_file_path = os.path.join(download_dir, file_path)\n",
        "\n",
        "  # Download the file\n",
        "  response = requests.get(url)\n",
        "  response.raise_for_status() # Raise an exception for bad status codes\n",
        "\n",
        "  with open(local_file_path, 'wb') as f:\n",
        "      f.write(response.content)\n",
        "\n",
        "  print(f\"Downloaded {file_path} to {local_file_path}\")\n",
        "\n",
        "  # Import the function from the downloaded file\n",
        "  spec = importlib.util.spec_from_file_location(\"extract_data_module\", local_file_path)\n",
        "  module = importlib.util.module_from_spec(spec)\n",
        "  spec.loader.exec_module(module)\n",
        "  # Call the extract_data function and print the output\n",
        "  return module.extract_data()"
      ],
      "metadata": {
        "id": "prjUcK7VXGll"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_features_year(data,year):\n",
        "    feature_map = {\n",
        "        'urbanization': 0,\n",
        "        'avg birth age': 1,\n",
        "        'happiness index': 2,\n",
        "        'health expenditure': 3,\n",
        "        'physicians per capita': 4,\n",
        "        'GNI PPP': 5,\n",
        "        'female labor participation': 6,\n",
        "        'christians (%)': 7,\n",
        "        'muslims (%)': 8,\n",
        "        'no religion (%)': 9,\n",
        "        'buddhists (%)': 10,\n",
        "        'hindus (%)': 11,\n",
        "        'jews (%)': 12,\n",
        "        'other religion (%)': 13,\n",
        "        'in Asia-Pacific':14,\n",
        "        'in Europe':15,\n",
        "        'in Latin America-Caribbean':16,\n",
        "        'in Middle East-North Africa':17,\n",
        "        'in North America':18,\n",
        "        'in Sub-Saharan Africa':19,\n",
        "        'post-HS education men (%)': 20,\n",
        "        'Post-HS education women (%)': 21,\n",
        "        'human development index': 22,\n",
        "        'gender inequality index': 23,\n",
        "        'first marriage age women': 24,\n",
        "        'first marriage age men': 25,\n",
        "        'maternity leave index': 26,\n",
        "        'work hours men': 27,\n",
        "        'work hours women': 28,\n",
        "        'abortion rate': 29,\n",
        "        'social media users': 30\n",
        "    }\n",
        "    countries = sorted(list(set([key[0] for key in data.keys()])))\n",
        "    df = pd.DataFrame(index=countries, columns=feature_map.keys())\n",
        "    target=[]\n",
        "    # Populate the DataFrame\n",
        "    for country in countries:\n",
        "        key = (country, year)\n",
        "        if key in data:\n",
        "            features = data[key][1]\n",
        "            target.append(data[key][0])\n",
        "            for feature_name, feature_index in feature_map.items():\n",
        "                if feature_index < len(features):\n",
        "                  df.loc[country, feature_name] = features[feature_index]\n",
        "                  if str(features[feature_index]) =='nan':\n",
        "                    df.loc[country, feature_name]=handle_missing_values(data,feature_index,country,year)\n",
        "    return df,target\n",
        "\n",
        "\n",
        "\n",
        "def handle_missing_values(data, feature_index, country, year):\n",
        "    # try taking the value from the last 3 years\n",
        "    for i in range(1, 4):\n",
        "        past_year = year - i\n",
        "        if (country, past_year) in data:\n",
        "            past_data = data[(country, past_year)][1] # Access the list of features\n",
        "            if feature_index < len(past_data) and str(past_data[feature_index])!='nan':\n",
        "                return past_data[feature_index]\n",
        "\n",
        "    # otherwise if missing last 3 years, linearly extrapulate from last decade\n",
        "    recent_years_data = []\n",
        "    for i in range(10,0,-1):\n",
        "        past_year = year - i\n",
        "        if (country, past_year) in data:\n",
        "            past_data = data[(country, past_year)][1] # Access the list of features\n",
        "            if feature_index < len(past_data) and not pd.isna(past_data[feature_index]):\n",
        "                 recent_years_data.append((past_year, past_data[feature_index]))\n",
        "\n",
        "    if len(recent_years_data) >= 2:\n",
        "        years = [item[0] for item in recent_years_data]\n",
        "        values = [item[1] for item in recent_years_data]\n",
        "        # Linear extrapolation using linear regression with polyfit\n",
        "        try:\n",
        "            m, c = np.polyfit(years, values, 1)\n",
        "            extrapolated_value = m * year + c\n",
        "            return extrapolated_value\n",
        "        except np.linalg.LinAlgError:\n",
        "            # Handle cases where polyfit fails (e.g., all years are the same)\n",
        "            return values[-1] # Return the last known value\n",
        "    elif len(recent_years_data) == 1:\n",
        "         # If only one data point in the last 10 years, use that value\n",
        "         return recent_years_data[0][1]\n",
        "\n",
        "    # If still missing after checking last 10 years, return NaN\n",
        "    return np.nan"
      ],
      "metadata": {
        "id": "cCWbIjgpEHDt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_missing_features(df_features):\n",
        "\n",
        "  # Identify countries with and without NaN values\n",
        "  countries_with_nan = df_features[df_features.isnull().any(axis=1)].index.tolist()\n",
        "  countries_without_nan = df_features.dropna().index.tolist()\n",
        "  countries_with_one_nan = df_features[df_features.isnull().sum(axis=1) == 1].index.tolist()\n",
        "\n",
        "  # Print the results\n",
        "  print(\"Countries with NaN values in their features:\")\n",
        "  print(countries_with_nan)\n",
        "  print(\"\\nCountries without NaN values in their features:\")\n",
        "  print(countries_without_nan)\n",
        "  print(\"\\nCountries with exactly one NaN value in their features:\")\n",
        "  print(countries_with_one_nan)\n",
        "\n",
        "  print(f\"\\nNumber of countries with NaN values: {len(countries_with_nan)}\")\n",
        "  print(f\"Number of countries without NaN values: {len(countries_without_nan)}\")\n",
        "  print(f\"Number of countries with exactly one NaN value: {len(countries_with_one_nan)}\")\n",
        "\n",
        "  # Group countries by their single missing feature\n",
        "  missing_features_grouped = {}\n",
        "  if countries_with_one_nan:\n",
        "      for country in countries_with_one_nan:\n",
        "          missing_feature_name = df_features.loc[country].isnull().idxmax()\n",
        "          if missing_feature_name not in missing_features_grouped:\n",
        "              missing_features_grouped[missing_feature_name] = []\n",
        "          missing_features_grouped[missing_feature_name].append(country)\n",
        "\n",
        "      # Print countries grouped by missing feature\n",
        "      print(\"\\nMissing feature for countries with exactly one NaN:\")\n",
        "      for feature, countries in missing_features_grouped.items():\n",
        "          print(f\"  Missing feature is '{feature}':\")\n",
        "          print(f\"    Countries: {', '.join(countries)}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4H0S2X3MyK0n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "process 2023 data:\n",
        "add some missing data, split to train and test and normalize"
      ],
      "metadata": {
        "id": "AHHje3EPAzwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_2023_data(data_output):\n",
        "    df_features, target = calc_features_year(data_output, 2023)\n",
        "    missing_values_to_fill = {\n",
        "        'abortion rate': {\n",
        "            'Cyprus': 7,'Ireland': 6.7,'Malaysia': 11,\n",
        "            'Mauritania': 42,'Malta': 3,'Morocco': 25, 'Congo, Rep.': 39\n",
        "        },\n",
        "        'maternity leave index': {\n",
        "            'Albania': 23.23,'Bosnia and Herzegovina': 52.14,\n",
        "            'Kazakhstan': 18,'Georgia': 7.59,'Kyrgyz Republic': 18,\n",
        "            'Moldova': 18,'Angola': 13,'Armenia': 20,\n",
        "            'Azerbaijan': 18,'Malawi': 12.86,'Bhutan': 8,\n",
        "            'Tanzania': 12,'Tajikistan': 20,'North Macedonia': 39,\n",
        "            'Liberia': 12.86,'Suriname': 0,'Uzbekistan': 18\n",
        "        },\n",
        "        'gender inequality index':{\n",
        "            'Central African Republic':0.682\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Fill in missing values using the dictionary\n",
        "    for feature, country_values in missing_values_to_fill.items():\n",
        "        for country, value in country_values.items():\n",
        "            if country in df_features.index:\n",
        "                df_features.loc[country, feature] = value\n",
        "\n",
        "    # Convert all columns to numeric, coercing errors\n",
        "    df_features = df_features.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    # Create a DataFrame for the target variable\n",
        "    y = pd.Series(target, index=df_features.index, name='target')\n",
        "\n",
        "    # Combine features and target into a single DataFrame for easier NaN handling\n",
        "    combined_df = pd.concat([df_features, y], axis=1)\n",
        "\n",
        "    # Remove rows with NaN values\n",
        "    combined_df_cleaned = combined_df.dropna()\n",
        "\n",
        "    # Separate features and target again\n",
        "    X = combined_df_cleaned.drop('target', axis=1)\n",
        "    y = combined_df_cleaned['target']\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "o11f5CwCA_bK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_features(X_train):\n",
        "\n",
        "  # Get the list of columns\n",
        "  columns = X_train.columns\n",
        "\n",
        "  # Calculate the number of rows needed\n",
        "  n_cols = 5\n",
        "  n_rows = math.ceil(len(columns) / n_cols)\n",
        "\n",
        "  # Create subplots\n",
        "  fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 5))\n",
        "  axes = axes.flatten() # Flatten the 2D array of axes for easy iteration\n",
        "\n",
        "  # Plot histograms for all features\n",
        "  for i, column in enumerate(columns):\n",
        "      axes[i].hist(X_train[column], bins=10)\n",
        "      axes[i].set_title(f'Histogram of {column}')\n",
        "      axes[i].set_xlabel(column)\n",
        "      axes[i].set_ylabel('Frequency')\n",
        "      axes[i].grid(True)\n",
        "\n",
        "  # Hide any unused subplots\n",
        "  for j in range(i + 1, len(axes)):\n",
        "      fig.delaxes(axes[j])\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-3EC63LtmKt7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Scaling2023(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self,log_features=None,features_to_scale=None,health_features=None):\n",
        "        self.log_features=log_features or ['no religion (%)','other religion (%)','buddhists (%)','hindus (%)','jews (%)','GNI PPP']\n",
        "        self.health_features=health_features or ['physicians per capita','health expenditure']\n",
        "        self.features_to_scale=features_to_scale or [\n",
        "            'urbanization','avg birth age','happiness index','GNI PPP','female labor participation','christians (%)','muslims (%)','no religion (%)',\n",
        "            'buddhists (%)','hindus (%)','jews (%)','other religion (%)','post-HS education men (%)','post-HS education men (%)',\n",
        "            'human development index','gender inequality index','first marriage age women','first marriage age men','maternity leave index',\n",
        "            'work hours men','work hours women','abortion rate','social media users','healthcare index score'\n",
        "        ]\n",
        "        self.health_scaler=StandardScaler()\n",
        "        self.scaler=StandardScaler()\n",
        "\n",
        "    def fit(self,X,y=None):\n",
        "        X_copy=X.copy()\n",
        "        missing=[c for c in (self.health_features+self.log_features) if c not in X_copy.columns]\n",
        "        if missing: raise KeyError(f\"Missing columns: {missing}\")\n",
        "\n",
        "        self.health_scaler.fit(X_copy[self.health_features])\n",
        "        h_scaled=self.health_scaler.transform(X_copy[self.health_features])\n",
        "        score=h_scaled[:,0]+h_scaled[:,1]\n",
        "        X_copy=X_copy.drop(columns=self.health_features)\n",
        "        X_copy['healthcare index score']=score\n",
        "        for f in self.log_features:\n",
        "            X_copy[f]=pd.to_numeric(X_copy[f])\n",
        "            X_copy[f]=np.log1p(X_copy[f])\n",
        "        self.scaler.fit(X_copy[self.features_to_scale])\n",
        "        return self\n",
        "\n",
        "    def transform(self,X):\n",
        "        X_copy=X.copy()\n",
        "        h_scaled=self.health_scaler.transform(X_copy[self.health_features])\n",
        "        score=h_scaled[:,0]+h_scaled[:,1]\n",
        "        X_copy=X_copy.drop(columns=self.health_features)\n",
        "        X_copy['healthcare index score']=score\n",
        "        for f in self.log_features:\n",
        "            X_copy[f]=pd.to_numeric(X_copy[f])\n",
        "            X_copy[f]=np.log1p(X_copy[f])\n",
        "        X_copy[self.features_to_scale]=self.scaler.transform(X_copy[self.features_to_scale])\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "fpk0R4uO_ChR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_health_index_score(X_train,X_test):\n",
        "    scaler_healthcare = StandardScaler()\n",
        "    health_features = ['physicians per capita','health expenditure']\n",
        "    X_train[health_features] = scaler_healthcare.fit_transform(X_train[health_features])\n",
        "    X_test[health_features] = scaler_healthcare.transform(X_test[health_features])\n",
        "    X_train['healthcare index score'] = X_train['physicians per capita'] + X_train['health expenditure']\n",
        "    X_test['healthcare index score'] = X_test['physicians per capita'] + X_test['health expenditure']\n",
        "    X_train = X_train.drop(['physicians per capita', 'health expenditure'], axis=1)\n",
        "    X_test = X_test.drop(['physicians per capita', 'health expenditure'], axis=1)\n",
        "    return X_train,X_test"
      ],
      "metadata": {
        "id": "IvTYKSwz_aR9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_correlations(df, method):\n",
        "    \"\"\"Calculates and visualizes correlation matrix, and prints top 5 correlated pairs.\"\"\"\n",
        "\n",
        "    correlation_matrix = df.corr(method=method)\n",
        "\n",
        "    plt.figure(figsize=(18, 15))\n",
        "    sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=\".2f\")\n",
        "    plt.title(f'{method.capitalize()} Correlation Heatmap of Features in X_train')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nTop 5 most highly correlated pairs of features (absolute {method.capitalize()} correlation):\")\n",
        "    upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
        "    stacked_corr = upper_tri.stack().sort_values(ascending=False, key=abs)\n",
        "    top_5_correlated_pairs = stacked_corr.head(5)\n",
        "\n",
        "    if top_5_correlated_pairs.empty:\n",
        "        print(\"No correlated pairs found.\")\n",
        "    else:\n",
        "        for (feature1, feature2), correlation in top_5_correlated_pairs.items():\n",
        "            print(f\"  {feature1} and {feature2}: {correlation:.4f}\")\n"
      ],
      "metadata": {
        "id": "hOqocTAfnYe2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_target_correlation(X, y, method_name):\n",
        "    combined_data = pd.concat([X, y], axis=1)\n",
        "    correlation_series = combined_data.corr(method=method_name)['target'].drop('target').sort_values(ascending=False)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x=correlation_series.values, y=correlation_series.index, palette='coolwarm')\n",
        "    plt.title(f'{method_name} Correlation of Features with Target Variable')\n",
        "    plt.xlabel(f'{method_name} Correlation Coefficient')\n",
        "    plt.ylabel('Features')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n{method_name} Correlation with Target:\")\n",
        "    print(correlation_series)\n"
      ],
      "metadata": {
        "id": "rP_0P9DHkpAd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_mutual_information(X, y):\n",
        "    \"\"\"Calculates and displays Mutual Information scores of features with the target.\"\"\"\n",
        "    mi_scores = mutual_info_regression(X, y)\n",
        "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
        "    mi_scores = mi_scores.sort_values(ascending=False)\n",
        "\n",
        "    print(\"Mutual Information Scores of Features with Target Variable:\")\n",
        "    print(mi_scores)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=mi_scores.values, y=mi_scores.index, palette='viridis')\n",
        "    plt.title('Mutual Information Scores of Features with Target Variable')\n",
        "    plt.xlabel('Mutual Information Score')\n",
        "    plt.ylabel('Features')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "iCz9f_EgB21d"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gridsearch_loocv_lasso(X,y):\n",
        "    pipe=Pipeline([('scaling2023',Scaling2023()),('lasso',Lasso())])\n",
        "    param_grid={'lasso__alpha':[val*(10**i) for i in range(-12,4) for val in [1,2,5]]}\n",
        "    gs=GridSearchCV(pipe,param_grid,cv=LeaveOneOut(),scoring='neg_mean_squared_error',n_jobs=-1)\n",
        "    gs.fit(X,y)\n",
        "    return gs\n",
        "\n",
        "def gridsearch_elastic_net_loocv(X,y):\n",
        "    pipe=Pipeline([('scaling2023',Scaling2023()),('elastic',ElasticNet())])\n",
        "    param_grid={\n",
        "        'elastic__alpha':[val*(10**i) for i in range(-6,2) for val in [1,2,5]],\n",
        "        'elastic__l1_ratio':[0.1,0.3,0.5,0.7,0.9,0.95,0.99,0.995,1.0]\n",
        "    }\n",
        "    gs=GridSearchCV(pipe,param_grid,cv=LeaveOneOut(),scoring='neg_mean_squared_error',n_jobs=-1)\n",
        "    gs.fit(X,y)\n",
        "    return gs\n",
        "\n",
        "def gridsearch_random_forest(X,y):\n",
        "  pipe=Pipeline([('scaling2023',Scaling2023()),('rf',RandomForestRegressor(random_state=42,n_jobs=-1))])\n",
        "  kf=KFold(n_splits=5,shuffle=True,random_state=42)\n",
        "  param_grid = {\n",
        "        'rf__n_estimators': [100, 200, 400],\n",
        "        'rf__max_depth': [None, 3, 4, 5],\n",
        "        'rf__min_samples_split': [2, 3 , 5, 9],\n",
        "        'rf__min_samples_leaf': [1, 2, 4, 6],\n",
        "        'rf__max_features': ['sqrt', 0.5, 1.0],\n",
        "    }\n",
        "  gs=GridSearchCV(pipe,param_grid,cv=kf,scoring='neg_mean_squared_error',n_jobs=-1)\n",
        "  gs.fit(X,y)\n",
        "  return gs\n",
        "\n",
        "\n",
        "def gridsearch_xgboost(X, y):\n",
        "    pipe = Pipeline([\n",
        "        ('scaling2023', Scaling2023()),\n",
        "        ('xgb', XGBRegressor(tree_method='hist',random_state=42,n_jobs=-1,importance_type='gain'))])\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    param_grid = {\n",
        "        'xgb__n_estimators': [40, 100, 300, 600],\n",
        "        'xgb__learning_rate': [0.003, 0.01 , 0.03, 0.1, 0.3],\n",
        "        'xgb__max_depth': [2, 3, 5 , 8],\n",
        "        'xgb__min_child_weight': [1, 3 , 5],\n",
        "        'xgb__subsample': [0.5, 0.8, 1.0],\n",
        "        'xgb__colsample_bytree': [0.5, 0.8, 1.0],\n",
        "        'xgb__gamma': [0, 1],\n",
        "    }\n",
        "    gs=GridSearchCV(pipe,param_grid,cv=kf,scoring='neg_mean_squared_error',n_jobs=-1)\n",
        "    gs.fit(X,y)\n",
        "    return gs\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def report_grid_search(gs,X,model):\n",
        "  best_est = gs.best_estimator_\n",
        "  names = best_est.named_steps['scaling2023'].transform(X).columns\n",
        "  idx = gs.best_index_\n",
        "  cv_mse = -gs.cv_results_['mean_test_score'][idx]\n",
        "  print(f\"Best params: {gs.best_params_}\")\n",
        "  print(f\"CV MSE: {cv_mse:.4f} (±{gs.cv_results_['std_test_score'][idx]:.4f})\")\n",
        "  est = best_est.named_steps[model]\n",
        "\n",
        "  if model in ['lasso','elastic']:\n",
        "    coefs = pd.Series(est.coef_, index=names)\n",
        "    coefs = coefs.reindex(coefs.abs().sort_values(ascending=False).index)\n",
        "    print(\"\\ncoefficients:\")\n",
        "    for f,c in coefs.items():\n",
        "      print(f\"{f}: {c:.4f}\")\n",
        "  else:#random forrest or xgboost\n",
        "    if model=='rf':\n",
        "      print(\"\\nmean decrease in imputirty by feature:\")\n",
        "    if model=='xgb':\n",
        "      print(\"\\nmean gain by feature:\")\n",
        "    imp = pd.Series(est.feature_importances_, index=names).sort_values(ascending=False)\n",
        "    for f,v in imp.items():\n",
        "      print(f\"{f}: {v:.4f}\")\n"
      ],
      "metadata": {
        "id": "9MNlG-ewC4zC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2023 data##\n",
        "X_train, X_test, y_train, y_test = process_2023_data(init_process())\n",
        "\n",
        "#feature distrebutions\n",
        "unscaled_X_train,unscaled_X_test = create_health_index_score(X_train.copy(),X_test.copy())\n",
        "print('unscaled features')\n",
        "plot_features(unscaled_X_train)\n",
        "\n",
        "scaler = Scaling2023()\n",
        "scaler.fit(X_train)\n",
        "scaled_train = scaler.transform(X_train)\n",
        "scaled_test = scaler.transform(X_test)\n",
        "\n",
        "print('scaled features')\n",
        "plot_features(scaled_train)\n",
        "\n",
        "#feature pairs correlations\n",
        "feature_correlations(scaled_train, 'pearson')\n",
        "feature_correlations(scaled_train, 'spearman')\n",
        "#feature-target correlations and mutual information\n",
        "analyze_target_correlation(scaled_train, y_train, 'pearson')\n",
        "analyze_target_correlation(scaled_train, y_train, 'spearman')\n",
        "analyze_mutual_information(scaled_train, y_train)\n",
        "#linear regressor models\n",
        "print('\\nlasso regressor:')\n",
        "lasso_gs = gridsearch_loocv_lasso(X_train,y_train)\n",
        "report_grid_search(lasso_gs,X_train,'lasso')\n",
        "print('\\nelastic net regressor:')\n",
        "elastic_net_gs = gridsearch_elastic_net_loocv(X_train,y_train)\n",
        "report_grid_search(elastic_net_gs,X_train,'elastic')\n",
        "#random forest regressor model\n",
        "print('\\nrandom forest regressor:')\n",
        "rf_gs = gridsearch_random_forest(X_train,y_train)\n",
        "report_grid_search(rf_gs,X_train,'rf')\n",
        "#xgboost regressor model\n",
        "print('\\nxgboost regressor:')\n",
        "xgb_gs = gridsearch_xgboost(X_train,y_train)\n",
        "report_grid_search(xgb_gs,X_train,'xgb')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "collapsed": true,
        "id": "8lHd4iGFxsFn",
        "outputId": "a124bf05-a6ab-4676-be7d-a2f4d4e371c1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded extract_data.py to data_extraction_code/extract_data.py\n",
            "Downloaded GNI_PPP_const_2021_dollars.csv to data/GNI_PPP_const_2021_dollars.csv\n",
            "Downloaded Religious-Composition-percentages.csv to data/Religious-Composition-percentages.csv\n",
            "Downloaded abortion-rates-by-country-2025.csv to data/abortion-rates-by-country-2025.csv\n",
            "Downloaded age-at-first-marriage-by-country-2025.csv to data/age-at-first-marriage-by-country-2025.csv\n",
            "Downloaded average-workweek-by-country-2025.csv to data/average-workweek-by-country-2025.csv\n",
            "Downloaded female-labor-force-participation-rates-slopes.csv to data/female-labor-force-participation-rates-slopes.csv\n",
            "Downloaded female-labor-participation.csv to data/female-labor-participation.csv\n",
            "Downloaded gender-inequality-index.xlsx to data/gender-inequality-index.xlsx\n",
            "Downloaded happiness-cantril-ladder.csv to data/happiness-cantril-ladder.csv\n",
            "Downloaded healthcare-expenditure-per-capita-ppp.csv to data/healthcare-expenditure-per-capita-ppp.csv\n",
            "Downloaded high_education_female.xlsx to data/high_education_female.xlsx\n",
            "Downloaded high_education_male.xlsx to data/high_education_male.xlsx\n",
            "Downloaded human_development_index.xlsx to data/human_development_index.xlsx\n",
            "Downloaded maternity-leave-by-country-2025.csv to data/maternity-leave-by-country-2025.csv\n",
            "Downloaded period-average-age-of-mothers.csv to data/period-average-age-of-mothers.csv\n",
            "Downloaded physicians-per-capita.csv to data/physicians-per-capita.csv\n",
            "Downloaded social-media-users-by-country-2025.csv to data/social-media-users-by-country-2025.csv\n",
            "Downloaded total-fertility-rate.csv to data/total-fertility-rate.csv\n",
            "Downloaded urban-population-share.csv to data/urban-population-share.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2288561048.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## 2023 data##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_2023_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#feature distrebutions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0munscaled_X_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munscaled_X_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_health_index_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2388930353.py\u001b[0m in \u001b[0;36minit_process\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;31m# Call the extract_data function and print the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/data_extraction_code/extract_data.py\u001b[0m in \u001b[0;36mextract_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    480\u001b[0m       \u001b[0mcountry_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Country Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myear_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m           \u001b[0mfertility_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfertility_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m               \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfertility_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1096\u001b[0m         \u001b[0mcheck_dict_or_set_indexers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.select_dtypes(include='object').columns.tolist())\n",
        "print('++++')\n",
        "print(X_train[['in Asia-Pacific', 'in Europe', 'in Latin America-Caribbean',\n",
        "         'in Middle East-North Africa', 'in North America', 'in Sub-Saharan Africa']].dtypes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnLABYZdzLw1",
        "outputId": "eabe9525-6572-49e3-82f5-bd6580e75e05"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['urbanization', 'avg birth age', 'happiness', 'health expenditure', 'physicians per capita', 'GNI PPP', 'female labor participation', 'christians', 'muslims', 'no religion', 'buddhists', 'hindus', 'jews', 'other religion', 'in Asia-Pacific', 'in Europe', 'in Latin America-Caribbean', 'in Middle East-North Africa', 'in North America', 'in Sub-Saharan Africa', 'education man', 'education women', 'hdi', 'gii', 'avg marriage age women', 'avg marriage age men', 'maternity leave index', 'work hours men', 'work hours women', 'abortions', 'social media users']\n",
            "++++\n",
            "in Asia-Pacific                object\n",
            "in Europe                      object\n",
            "in Latin America-Caribbean     object\n",
            "in Middle East-North Africa    object\n",
            "in North America               object\n",
            "in Sub-Saharan Africa          object\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Assuming grid_search_rf, X_train, y_train, X_test, and y_test are already defined\n",
        "\n",
        "# Get the best trained Random Forest model\n",
        "best_rf_model = rf_gs.best_estimator_\n",
        "\n",
        "# --- Evaluate on Training Set ---\n",
        "y_train_pred_rf = best_rf_model.predict(X_train)\n",
        "mse_train_rf = mean_squared_error(y_train, y_train_pred_rf)\n",
        "r2_train_rf = r2_score(y_train, y_train_pred_rf)\n",
        "\n",
        "print(f\"Training Set Performance:\")\n",
        "print(f\"  Mean Squared Error: {mse_train_rf:.4f}\")\n",
        "print(f\"  R-squared: {r2_train_rf:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# --- Evaluate on Test Set ---\n",
        "y_test_pred_rf = best_rf_model.predict(X_test)\n",
        "mse_test_rf = mean_squared_error(y_test, y_test_pred_rf)\n",
        "r2_test_rf = r2_score(y_test, y_test_pred_rf)\n",
        "\n",
        "print(f\"Test Set Performance:\")\n",
        "print(f\"  Mean Squared Error: {mse_test_rf:.4f}\")\n",
        "print(f\"  R-squared: {r2_test_rf:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# --- Validation Set Performance (from Grid Search) ---\n",
        "# The best_score_ from GridSearchCV is the mean cross-validation score (negative MSE)\n",
        "validation_neg_mse = rf_gs.best_score_\n",
        "validation_mse = -validation_neg_mse # Convert negative MSE to positive MSE\n",
        "\n",
        "print(f\"Validation Set Performance (from Grid Search):\")\n",
        "print(f\"  Mean Cross-Validation MSE: {validation_mse:.4f}\")\n",
        "# Note: R-squared is not directly available from best_score_ if MSE was the scoring metric\n",
        "# You could calculate R-squared on the validation folds separately if needed."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7d6P0I32lOw",
        "outputId": "df179eba-67d1-4a7c-c6ce-ed2d338b1dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Performance:\n",
            "  Mean Squared Error: 0.0740\n",
            "  R-squared: 0.9602\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Set Performance:\n",
            "  Mean Squared Error: 0.1473\n",
            "  R-squared: 0.8882\n",
            "\n",
            "==================================================\n",
            "\n",
            "Validation Set Performance (from Grid Search):\n",
            "  Mean Cross-Validation MSE: 0.3397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_G-9gBjwS-X0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}